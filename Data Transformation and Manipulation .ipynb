{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation and Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity we will be manpulating data from a business. We will be using four datasets, corresponding to three departments,a fourth one containing the managers of the business, and a fifth one that contains all of the employees, but has different data on them. Our goal is to put all of these together in one dataset, so it is easier for the business management.\n",
    "\n",
    "\n",
    "### How does this notebook work?\n",
    "\n",
    "Run the cells with code written on them. To do this, you can select them and press Shift + Enter or press the \"Play\" button on the left side of the cell (if you do not see this, hover with the mouse on the cell and it should appear). Remember that all cells need to be run in order, even those in which you did not need to write any code. If you think you might have skipped one, on the left side of the cell you will find the line number, which corresponds to the order in which you have ran the cells. Keep in mind that if you run a cell and it gets numbered as 4, if you run the same cell immediately after it will be renumbered to 5.\n",
    "\n",
    "Looking at the code you will notice some parts are incomplete and have '\\_\\_' written instead. This means that you need to complete that part of the code. In some other parts, you will need to write your own code. This will be specified.\n",
    "\n",
    "You will also find [hyperlinks]() to documentation on different functions that we will use. It is recommended to look at them to familiarise yourself with what you are doing and how they work.\n",
    "\n",
    "There are also questions ***in bold*** to be completed in text cells. Click twice on them to start editing them or select them and press Enter, and press Shift+Enter when you are finished to go back to reading mode. The questions can be answered in one or two sentences in general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import os\n",
    "import matplotlib.pyplot \n",
    "import math\n",
    "import seaborn \n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have links to the libraries we have imported:\n",
    "\n",
    "[OS library](https://docs.python.org/3/library/os.html): Library with different useful functions to interact with the Operating System. We will be using this for loading the datasets.\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/): Pandas is a tool for data analysis and manipulation.\n",
    "\n",
    "[Seaborn](https://seaborn.pydata.org/): Seaborn is a library specialised in statistical data visualisation.\n",
    "\n",
    "[Numpy](https://numpy.org/): Numpy is the Python library for mathematics. We will use it for performing operations on our data.\n",
    "\n",
    "[Matplotlib](https://matplotlib.org/): Matplotlib is another library specilised in visualisation for Python.\n",
    "\n",
    "[Math module](https://docs.python.org/3/library/math.html): This module provides mathematical functions defined by the C standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the datasets. \n",
    "\n",
    "For this we have to get the path to the data we are using, and then read the file. In this case, it is in the datasets folder inside our working directory called 'datasets', and the files that we will be using are the following:\n",
    "    \n",
    "    - it_department.csv: Contains the employees in the IT department.\n",
    "    - hr_department.csv: Contains the employees in the Human Resources department.\n",
    "    - sales_department.csv: Contains the employees in the Sales department.\n",
    "    - managers.csv: Contains the managers of the business.\n",
    "    - employees.csv: Contains all the people who work in the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the path to the files we will be using.\n",
    "path_it = os.path.join(os.getcwd(), 'datasets', 'it_department.csv')\n",
    "path_hr = os.path.join(os.getcwd(), 'datasets', 'hr_department.csv')\n",
    "path_sales = os.path.join(os.getcwd(), 'datasets', 'sales_department.csv')\n",
    "path_managers = os.path.join(os.getcwd(), 'datasets', 'managers.csv')\n",
    "path_employees = os.path.join(os.getcwd(), 'datasets', 'employees.csv')\n",
    "\n",
    "#Load the data into the countries_info variable. This results in a DataFrame object.\n",
    "it_dpt = pandas.read_csv(path_it, delimiter = ';')\n",
    "hr_dpt = pandas.read_csv(path_hr, delimiter = ';')\n",
    "sales_dpt = pandas.read_csv(path_sales, delimiter = ';')\n",
    "managers = pandas.read_csv(path_managers, delimiter = ';')\n",
    "employees = pandas.read_csv(path_employees, delimiter = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at the attributes of each of the datasets that we have loaded using the [.columns](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html) attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(it_dpt.columns)\n",
    "print(hr_dpt.columns)\n",
    "print(sales_dpt.columns)\n",
    "print(managers.columns)\n",
    "print(employees.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Do all the files contain the same attributes? Which file(s) is different to the others?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the files for each of the departments, but we only know which department they belong to by the name of their variable. As we will be putting together all the datasets, we do not want to lose this information. To avoid this, we will now add a column to the three department datasets with the name of the department. To add a column in the dataframe, we just need to give it a name and a list of values to assign. If we use only one value, then this will be applied to all the columns in that dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a column to the IT department dataset\n",
    "it_dpt['department'] = 'it'\n",
    "#Do the same as above to add a column for the sales and HR departments\n",
    "hr_dpt['department'] = '__'\n",
    "sales_dpt['department'] = '__'\n",
    "\n",
    "#Look at the HR file to check that the new column has been added\n",
    "hr_dpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The employees dataset contains information that the other files do not. If we wanted to join the IT department's information with the employee's, we would need to find an attribute that is common in both datasets. ***Looking at the column names we extracted earlier, is there any common attribute that would allow us to do this, ie: a key?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the employees would be our key in case of joining the datasets. It would be useful to the employer to have all the employee information in one file. There are two joining techniques: appending and merging. The first one adds one dataset to the end of the other, and merging adds new columns and uses a key to do this.\n",
    "\n",
    "***In this case, if we wanted to create a dataset with all the information for each employee, which joining technique should we use?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to merge the datasets, we need to check that they have the right size and that we would not be loosing any data on the way. A way to do this is to check the number of unique names in the employees file and compare it to that of the other files together through the function [nunique()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nunique.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of names in the employees data\n",
    "num_employees = employees['name'].nunique()\n",
    "print (\"There are \",num_employees, \" unique names in the employee file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count how many unique names there are in all of the data files, we can put them together as one and then use again the nunique() function. As this function only counts how many unique elements there are, we do not need to worry if the same name is in more than one data file, it will only be counted once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put them together\n",
    "appended_dpts = it_dpt.append(hr_dpt)\n",
    "appended_dpts = appended_dpts.append(sales_dpt)\n",
    "appended_dpts = appended_dpts.append(managers)\n",
    "#Count how many there are\n",
    "print(\"There are \",appended_dpts['name'].nunique(dropna=True), \" unique names in all the files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now seen that the numbers are the same. But it could be that there are different names in both lists. To check for this, we see if the list of unique names from the general list is contained in the appended one with the [issubset()](https://www.w3schools.com/python/ref_set_issubset.asp) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the key we decided on earlier \n",
    "set(employees['__']).issubset(set(appended_dpts['__']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that they are the same, we can merge them into a bigger dataset with the information of all the workers. Recall the following types of merge:\n",
    "\n",
    "    - Left join\n",
    "    - Right join\n",
    "    - Inner join\n",
    "    - Outer join/ Full join\n",
    "\n",
    "Given these, ***which type of merge do you think would be appropiate for this case?***\n",
    "An outer join would be better. It could be the case that an employee does not show in one of the files, and if it was an inner join their information would be lost.\n",
    "\n",
    "We will use the Pandas function [merge()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the merge here. Write the key in the on argument\n",
    "merged_data = employees.merge(appended_dpts, on='__', how='outer')\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some of the employees appear twice, and have exactly the same information. When we appended the datasets earlier, we did not check for employees that could appear in more than one dataset. We will use the function [drop_duplicates()]() to delete those entries that are duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the duplicates\n",
    "merged_data = merged_data.drop_duplicates()\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the data of all the employees in one dataset. This will be useful in case some analysis needs to be done in the business in general, like a study on the salaries of its employees. Merging is an important technique when working on datasets, but it is important to use it when necessary and keep in mind that datasets can grow very fast if the ones being merged are big. One of the ways to avoid this is to check for duplicate data, as we did."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
