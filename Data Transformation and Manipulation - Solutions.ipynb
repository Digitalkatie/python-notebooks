{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation and Manipulation - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity we will be manpulating data from a business. We will be using four datasets, corresponding to three departments,a fourth one containing the managers of the business, and a fifth one that contains all of the employees, but has different data on them. Our goal is to put all of these together in one dataset, so it is easier for the business management.\n",
    "\n",
    "\n",
    "### How does this notebook work?\n",
    "\n",
    "Run the cells with code written on them. To do this, you can select them and press Shift + Enter or press the \"Play\" button on the left side of the cell (if you do not see this, hover with the mouse on the cell and it should appear). Remember that all cells need to be run in order, even those in which you did not need to write any code. If you think you might have skipped one, on the left side of the cell you will find the line number, which corresponds to the order in which you have ran the cells. Keep in mind that if you run a cell and it gets numbered as 4, if you run the same cell immediately after it will be renumbered to 5.\n",
    "\n",
    "Looking at the code you will notice some parts are incomplete and have '\\_\\_' written instead. This means that you need to complete that part of the code. In some other parts, you will need to write your own code. This will be specified.\n",
    "\n",
    "You will also find [hyperlinks]() to documentation on different functions that we will use. It is recommended to look at them to familiarise yourself with what you are doing and how they work.\n",
    "\n",
    "There are also questions to be completed in text cells. Click twice on them to start editing them or select them and press Enter, and press Shift+Enter when you are finished to go back to reading mode. The questions can be answered in one or two sentences in general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import os\n",
    "import matplotlib.pyplot \n",
    "import math\n",
    "import seaborn \n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have links to the libraries we have imported:\n",
    "\n",
    "[OS library](https://docs.python.org/3/library/os.html): Library with different useful functions to interact with the Operating System. We will be using this for loading the datasets.\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/): Pandas is a tool for data analysis and manipulation.\n",
    "\n",
    "[Seaborn](https://seaborn.pydata.org/): Seaborn is a library specialised in statistical data visualisation.\n",
    "\n",
    "[Numpy](https://numpy.org/): Numpy is the Python library for mathematics. We will use it for performing operations on our data.\n",
    "\n",
    "[Matplotlib](https://matplotlib.org/): Matplotlib is another library specilised in visualisation for Python.\n",
    "\n",
    "[Math module](https://docs.python.org/3/library/math.html): This module provides mathematical functions defined by the C standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the datasets. \n",
    "\n",
    "For this we have to get the path to the data we are using, and then read the file. In this case, it is in the datasets folder inside our working directory called 'datasets', and the files that we will be using are the following:\n",
    "    \n",
    "    - it_department.csv: Contains the employees in the IT department.\n",
    "    - hr_department.csv: Contains the employees in the Human Resources department.\n",
    "    - sales_department.csv: Contains the employees in the Sales department.\n",
    "    - managers.csv: Contains the managers of the business.\n",
    "    - employees.csv: Contains all the people who work in the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the path to the files we will be using.\n",
    "path_it = os.path.join(os.getcwd(), 'datasets', 'it_department.csv')\n",
    "path_hr = os.path.join(os.getcwd(), 'datasets', 'hr_department.csv')\n",
    "path_sales = os.path.join(os.getcwd(), 'datasets', 'sales_department.csv')\n",
    "path_managers = os.path.join(os.getcwd(), 'datasets', 'managers.csv')\n",
    "path_employees = os.path.join(os.getcwd(), 'datasets', 'employees.csv')\n",
    "\n",
    "#Load the data into the countries_info variable. This results in a DataFrame object.\n",
    "it_dpt = pandas.read_csv(path_it, delimiter = ';')\n",
    "hr_dpt = pandas.read_csv(path_hr, delimiter = ';')\n",
    "sales_dpt = pandas.read_csv(path_sales, delimiter = ';')\n",
    "managers = pandas.read_csv(path_managers, delimiter = ';')\n",
    "employees = pandas.read_csv(path_employees, delimiter = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at the attributes of each of the datasets that we have loaded using the [.columns](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html) attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'office', 'phone_extension'], dtype='object')\n",
      "Index(['name', 'office', 'phone_extension'], dtype='object')\n",
      "Index(['name', 'office', 'phone_extension'], dtype='object')\n",
      "Index(['name', 'office', 'phone_extension', 'department'], dtype='object')\n",
      "Index(['name', 'monthly_salary', 'birthdate', 'gender', 'manager'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(it_dpt.columns)\n",
    "print(hr_dpt.columns)\n",
    "print(sales_dpt.columns)\n",
    "print(managers.columns)\n",
    "print(employees.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Do all the files contain the same attributes? Which file(s) is different to the others?***\n",
    "\n",
    "The managers file has a 'department' column that the others do not, and the employees file has different attributes except for the name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the files for each of the departments, but we only know which department they belong to by the name of their variable. As we will be putting together all the datasets, we do not want to lose this information. To avoid this, we will now add a column to the three department datasets with the name of the department. To add a column in the dataframe, we just need to give it a name and a list of values to assign. If we use only one value, then this will be applied to all the columns in that dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>office</th>\n",
       "      <th>phone_extension</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nicole Vincent</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1110</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maisha Hughes</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1120</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Omer Robin</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1111</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dawid Wu</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1130</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jamie Do</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1140</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  office  phone_extension department\n",
       "0  Nicole Vincent    1.11             1110         hr\n",
       "1   Maisha Hughes    1.12             1120         hr\n",
       "2      Omer Robin    1.11             1111         hr\n",
       "3        Dawid Wu    1.13             1130         hr\n",
       "4        Jamie Do    1.14             1140         hr"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add a column to the IT department dataset\n",
    "it_dpt['department'] = 'it'\n",
    "#Do the same as above to add a column for the sales and HR departments\n",
    "hr_dpt['department'] = 'hr'\n",
    "sales_dpt['department'] = 'sales'\n",
    "\n",
    "#Look at the HR file to check that the new column has been added\n",
    "hr_dpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The employees dataset contains information that the other files do not. If we wanted to join the IT department's information with the employee's, we would need to find an attribute that is common in both datasets. ***Looking at the column names we extracted earlier, is there any common attribute that would allow us to do this, ie: a key?***\n",
    "\n",
    "The key or common attribute would be the name of the employee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the employees would be our key in case of joining the datasets. It would be useful to the employer to have all the employee information in one file. There are two joining techniques: appending and merging. The first one adds one dataset to the end of the other, and merging adds new columns and uses a key to do this.\n",
    "\n",
    "***In this case, if we wanted to create a dataset with all the information for each employee, which joining technique should we use?***\n",
    "\n",
    "We need to merge the datasets because we have different information about each employee in the employees file and the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to merge the datasets, we need to check that they have the right size and that we would not be loosing any data on the way. A way to do this is to check the number of unique names in the employees file and compare it to that of the other files together through the function [nunique()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nunique.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  17  unique names in the employee file\n"
     ]
    }
   ],
   "source": [
    "#Get the number of names in the employees data\n",
    "num_employees = employees['name'].nunique()\n",
    "print (\"There are \",num_employees, \" unique names in the employee file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count how many unique names there are in all of the data files, we can put them together as one and then use again the nunique() function. As this function only counts how many unique elements there are, we do not need to worry if the same name is in more than one data file, it will only be counted once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  17  unique names in all the files\n"
     ]
    }
   ],
   "source": [
    "#Put them together\n",
    "appended_dpts = it_dpt.append(hr_dpt)\n",
    "appended_dpts = appended_dpts.append(sales_dpt)\n",
    "appended_dpts = appended_dpts.append(managers)\n",
    "#Count how many there are\n",
    "print(\"There are \",appended_dpts['name'].nunique(dropna=True), \" unique names in all the files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now seen that the numbers are the same. But it could be that there are different names in both lists. To check for this, we see if the list of unique names from the general list is contained in the appended one with the [issubset()](https://www.w3schools.com/python/ref_set_issubset.asp) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write the key we decided on earlier \n",
    "set(employees['name']).issubset(set(appended_dpts['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that they are the same, we can merge them into a bigger dataset with the information of all the workers. Recall the following types of merge:\n",
    "\n",
    "    - Left join\n",
    "    - Right join\n",
    "    - Inner join\n",
    "    - Outer join/ Full join\n",
    "\n",
    "Given these, ***which type of merge do you think would be appropiate for this case?***\n",
    "An outer join would be better. It could be the case that an employee does not show in one of the files, and if it was an inner join their information would be lost.\n",
    "\n",
    "We will use the Pandas function [merge()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>monthly_salary</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>gender</th>\n",
       "      <th>manager</th>\n",
       "      <th>office</th>\n",
       "      <th>phone_extension</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jamie Do</td>\n",
       "      <td>1800</td>\n",
       "      <td>04/04/1980</td>\n",
       "      <td>NB</td>\n",
       "      <td>Betty</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1140</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jamie Do</td>\n",
       "      <td>1800</td>\n",
       "      <td>04/04/1980</td>\n",
       "      <td>NB</td>\n",
       "      <td>Betty</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1140</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice Duran</td>\n",
       "      <td>1700</td>\n",
       "      <td>03/12/1969</td>\n",
       "      <td>F</td>\n",
       "      <td>Betty</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3120</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice Duran</td>\n",
       "      <td>1700</td>\n",
       "      <td>03/12/1969</td>\n",
       "      <td>F</td>\n",
       "      <td>Betty</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3120</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob Gill</td>\n",
       "      <td>1800</td>\n",
       "      <td>23/02/1970</td>\n",
       "      <td>M</td>\n",
       "      <td>Betty</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2120</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bob Gill</td>\n",
       "      <td>1800</td>\n",
       "      <td>23/02/1970</td>\n",
       "      <td>M</td>\n",
       "      <td>Betty</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2120</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Betty Dolan</td>\n",
       "      <td>2500</td>\n",
       "      <td>25/06/1963</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2150</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nicole Vincent</td>\n",
       "      <td>1500</td>\n",
       "      <td>25/06/1963</td>\n",
       "      <td>F</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1110</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Maisha Hughes</td>\n",
       "      <td>1500</td>\n",
       "      <td>07/03/1975</td>\n",
       "      <td>F</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1120</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Omer Robin</td>\n",
       "      <td>1500</td>\n",
       "      <td>25/02/1977</td>\n",
       "      <td>M</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1111</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dawid Wu</td>\n",
       "      <td>1500</td>\n",
       "      <td>21/09/1968</td>\n",
       "      <td>M</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1130</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lula Mendez</td>\n",
       "      <td>1500</td>\n",
       "      <td>05/10/1955</td>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3140</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Charles Tucker</td>\n",
       "      <td>1500</td>\n",
       "      <td>21/12/1989</td>\n",
       "      <td>M</td>\n",
       "      <td>Alice</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3141</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Alex Yaar</td>\n",
       "      <td>1500</td>\n",
       "      <td>19/11/1969</td>\n",
       "      <td>NB</td>\n",
       "      <td>Alice</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3150</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lewis Craft</td>\n",
       "      <td>1500</td>\n",
       "      <td>08/10/1973</td>\n",
       "      <td>M</td>\n",
       "      <td>Alice</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3110</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Vihaan Donaldson</td>\n",
       "      <td>1500</td>\n",
       "      <td>15/05/1985</td>\n",
       "      <td>M</td>\n",
       "      <td>Alice</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3111</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Isaac Byrne</td>\n",
       "      <td>1500</td>\n",
       "      <td>22/03/1962</td>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2100</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Nyota Uhura</td>\n",
       "      <td>1500</td>\n",
       "      <td>19/01/1978</td>\n",
       "      <td>F</td>\n",
       "      <td>Bob</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2130</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Jadzia Dax</td>\n",
       "      <td>1500</td>\n",
       "      <td>27/04/1985</td>\n",
       "      <td>F</td>\n",
       "      <td>Bob</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2131</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Erika Bouvet</td>\n",
       "      <td>1500</td>\n",
       "      <td>05/08/1969</td>\n",
       "      <td>F</td>\n",
       "      <td>Bob</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2101</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  monthly_salary   birthdate gender manager  office  \\\n",
       "0           Jamie Do            1800  04/04/1980     NB   Betty    1.14   \n",
       "1           Jamie Do            1800  04/04/1980     NB   Betty    1.14   \n",
       "2        Alice Duran            1700  03/12/1969      F   Betty    3.12   \n",
       "3        Alice Duran            1700  03/12/1969      F   Betty    3.12   \n",
       "4           Bob Gill            1800  23/02/1970      M   Betty    2.12   \n",
       "5           Bob Gill            1800  23/02/1970      M   Betty    2.12   \n",
       "6        Betty Dolan            2500  25/06/1963      F     NaN    2.15   \n",
       "7     Nicole Vincent            1500  25/06/1963      F   Jamie    1.11   \n",
       "8      Maisha Hughes            1500  07/03/1975      F   Jamie    1.12   \n",
       "9         Omer Robin            1500  25/02/1977      M   Jamie    1.11   \n",
       "10          Dawid Wu            1500  21/09/1968      M   Jamie    1.13   \n",
       "11       Lula Mendez            1500  05/10/1955      F   Alice    3.14   \n",
       "12    Charles Tucker            1500  21/12/1989      M   Alice    3.14   \n",
       "13         Alex Yaar            1500  19/11/1969     NB   Alice    3.15   \n",
       "14       Lewis Craft            1500  08/10/1973      M   Alice    3.11   \n",
       "15  Vihaan Donaldson            1500  15/05/1985      M   Alice    3.11   \n",
       "16       Isaac Byrne            1500  22/03/1962      M     Bob    2.10   \n",
       "17       Nyota Uhura            1500  19/01/1978      F     Bob    2.13   \n",
       "18        Jadzia Dax            1500  27/04/1985      F     Bob    2.13   \n",
       "19      Erika Bouvet            1500  05/08/1969      F     Bob    2.10   \n",
       "\n",
       "    phone_extension department  \n",
       "0              1140         hr  \n",
       "1              1140         hr  \n",
       "2              3120         it  \n",
       "3              3120         it  \n",
       "4              2120      sales  \n",
       "5              2120      sales  \n",
       "6              2150        NaN  \n",
       "7              1110         hr  \n",
       "8              1120         hr  \n",
       "9              1111         hr  \n",
       "10             1130         hr  \n",
       "11             3140         it  \n",
       "12             3141         it  \n",
       "13             3150         it  \n",
       "14             3110         it  \n",
       "15             3111         it  \n",
       "16             2100      sales  \n",
       "17             2130      sales  \n",
       "18             2131      sales  \n",
       "19             2101      sales  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do the merge here\n",
    "merged_data = employees.merge(appended_dpts, on='name', how='outer')\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some of the employees appear twice, and have exactly the same information. When we appended the datasets earlier, we did not check for employees that could appear in more than one dataset. We will use the function [drop_duplicates()]() to delete those entries that are duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>monthly_salary</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>gender</th>\n",
       "      <th>manager</th>\n",
       "      <th>office</th>\n",
       "      <th>phone_extension</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jamie Do</td>\n",
       "      <td>1800</td>\n",
       "      <td>04/04/1980</td>\n",
       "      <td>NB</td>\n",
       "      <td>Betty</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1140</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice Duran</td>\n",
       "      <td>1700</td>\n",
       "      <td>03/12/1969</td>\n",
       "      <td>F</td>\n",
       "      <td>Betty</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3120</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob Gill</td>\n",
       "      <td>1800</td>\n",
       "      <td>23/02/1970</td>\n",
       "      <td>M</td>\n",
       "      <td>Betty</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2120</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Betty Dolan</td>\n",
       "      <td>2500</td>\n",
       "      <td>25/06/1963</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2150</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nicole Vincent</td>\n",
       "      <td>1500</td>\n",
       "      <td>25/06/1963</td>\n",
       "      <td>F</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1110</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Maisha Hughes</td>\n",
       "      <td>1500</td>\n",
       "      <td>07/03/1975</td>\n",
       "      <td>F</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1120</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Omer Robin</td>\n",
       "      <td>1500</td>\n",
       "      <td>25/02/1977</td>\n",
       "      <td>M</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1111</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dawid Wu</td>\n",
       "      <td>1500</td>\n",
       "      <td>21/09/1968</td>\n",
       "      <td>M</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1130</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lula Mendez</td>\n",
       "      <td>1500</td>\n",
       "      <td>05/10/1955</td>\n",
       "      <td>F</td>\n",
       "      <td>Alice</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3140</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Charles Tucker</td>\n",
       "      <td>1500</td>\n",
       "      <td>21/12/1989</td>\n",
       "      <td>M</td>\n",
       "      <td>Alice</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3141</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Alex Yaar</td>\n",
       "      <td>1500</td>\n",
       "      <td>19/11/1969</td>\n",
       "      <td>NB</td>\n",
       "      <td>Alice</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3150</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lewis Craft</td>\n",
       "      <td>1500</td>\n",
       "      <td>08/10/1973</td>\n",
       "      <td>M</td>\n",
       "      <td>Alice</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3110</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Vihaan Donaldson</td>\n",
       "      <td>1500</td>\n",
       "      <td>15/05/1985</td>\n",
       "      <td>M</td>\n",
       "      <td>Alice</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3111</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Isaac Byrne</td>\n",
       "      <td>1500</td>\n",
       "      <td>22/03/1962</td>\n",
       "      <td>M</td>\n",
       "      <td>Bob</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2100</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Nyota Uhura</td>\n",
       "      <td>1500</td>\n",
       "      <td>19/01/1978</td>\n",
       "      <td>F</td>\n",
       "      <td>Bob</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2130</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Jadzia Dax</td>\n",
       "      <td>1500</td>\n",
       "      <td>27/04/1985</td>\n",
       "      <td>F</td>\n",
       "      <td>Bob</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2131</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Erika Bouvet</td>\n",
       "      <td>1500</td>\n",
       "      <td>05/08/1969</td>\n",
       "      <td>F</td>\n",
       "      <td>Bob</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2101</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  monthly_salary   birthdate gender manager  office  \\\n",
       "0           Jamie Do            1800  04/04/1980     NB   Betty    1.14   \n",
       "2        Alice Duran            1700  03/12/1969      F   Betty    3.12   \n",
       "4           Bob Gill            1800  23/02/1970      M   Betty    2.12   \n",
       "6        Betty Dolan            2500  25/06/1963      F     NaN    2.15   \n",
       "7     Nicole Vincent            1500  25/06/1963      F   Jamie    1.11   \n",
       "8      Maisha Hughes            1500  07/03/1975      F   Jamie    1.12   \n",
       "9         Omer Robin            1500  25/02/1977      M   Jamie    1.11   \n",
       "10          Dawid Wu            1500  21/09/1968      M   Jamie    1.13   \n",
       "11       Lula Mendez            1500  05/10/1955      F   Alice    3.14   \n",
       "12    Charles Tucker            1500  21/12/1989      M   Alice    3.14   \n",
       "13         Alex Yaar            1500  19/11/1969     NB   Alice    3.15   \n",
       "14       Lewis Craft            1500  08/10/1973      M   Alice    3.11   \n",
       "15  Vihaan Donaldson            1500  15/05/1985      M   Alice    3.11   \n",
       "16       Isaac Byrne            1500  22/03/1962      M     Bob    2.10   \n",
       "17       Nyota Uhura            1500  19/01/1978      F     Bob    2.13   \n",
       "18        Jadzia Dax            1500  27/04/1985      F     Bob    2.13   \n",
       "19      Erika Bouvet            1500  05/08/1969      F     Bob    2.10   \n",
       "\n",
       "    phone_extension department  \n",
       "0              1140         hr  \n",
       "2              3120         it  \n",
       "4              2120      sales  \n",
       "6              2150        NaN  \n",
       "7              1110         hr  \n",
       "8              1120         hr  \n",
       "9              1111         hr  \n",
       "10             1130         hr  \n",
       "11             3140         it  \n",
       "12             3141         it  \n",
       "13             3150         it  \n",
       "14             3110         it  \n",
       "15             3111         it  \n",
       "16             2100      sales  \n",
       "17             2130      sales  \n",
       "18             2131      sales  \n",
       "19             2101      sales  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the duplicates\n",
    "merged_data = merged_data.drop_duplicates()\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the data of all the employees in one dataset. This will be useful in case some analysis needs to be done in the business in general, like a study on the salaries of its employees. Merging is an important technique when working on datasets, but it is important to use it when necessary and keep in mind that datasets can grow very fast if the ones being merged are big. One of the ways to avoid this is to check for duplicate data, as we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
